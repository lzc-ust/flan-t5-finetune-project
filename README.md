# Flan-T5 Fine-tuning for Software Engineering QA

This project fine-tunes [Flan-T5](https://huggingface.co/google/flan-t5-large) on a curated dataset of software engineering interview questions. It supports local training, evaluation, and batch inference, and was further scaled using the HKUST SuperPOD GPU cluster.

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ checkpoints/                 # Saved model checkpoints (auto-generated)
â”œâ”€â”€ datasets/                   # Processed training/validation/test data and predictions
â”‚   â”œâ”€â”€ train.json              # Training data (80%)
â”‚   â”œâ”€â”€ valid.json              # Validation data (10%)
â”‚   â”œâ”€â”€ test.json               # Test data (10%)
â”‚   â”œâ”€â”€ predicted_answers_local.jsonl  # Model outputs on test set
â”‚   â”œâ”€â”€ predicted_answers_local.csv
â”‚   â””â”€â”€ Software Questions.csv  # Original dataset before formatting
â”œâ”€â”€ logs/                       # Slurm output/error logs from SuperPOD
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_flan_t5.slurm     # SLURM script for training on SuperPOD
â”œâ”€â”€ src/                        # Core source code
â”‚   â”œâ”€â”€ train_flan_t5.py        # Training script (for general use)
â”‚   â”œâ”€â”€ train_flan_t5_local.py  # Training script (local GPU environment)
â”‚   â”œâ”€â”€ preprocess_dataset.py   # Data formatting and splitting
â”‚   â”œâ”€â”€ inference_flan_t5_local.py    # Run inference on test set
â”‚   â”œâ”€â”€ inference_to_csv_local.py     # Convert predictions to CSV
â”‚   â””â”€â”€ compute_metrics.py      # Compute BLEU, ROUGE, METEOR
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ run_eval_testset.sh     # Evaluation pipeline
â”œâ”€â”€ init_env.sh                # Conda environment initialization
â”œâ”€â”€ requirements.txt           # Main dependencies
â”œâ”€â”€ requirements-freeze.txt    # Frozen environment (pip freeze)
â””â”€â”€ README.md                  # You're here :)
```

---

## ğŸš€ Setup Instructions

### 1. Create and activate environment

```bash
bash init_env.sh
conda activate flan-t5-finetune
```

Or manually:

```bash
conda create -n flan-t5-finetune python=3.10
pip install -r requirements.txt
```

---

## ğŸ“¦ Data Preparation

To preprocess and split the dataset:

```bash
python src/preprocess_dataset.py
```

This will generate:

- `train.json`, `valid.json`, `test.json` under `datasets/`
- Additional `.jsonl` and `.csv` files for inference output

---

## ğŸ‹ï¸ Model Training

### âœ… Local GPU

```bash
python src/train_flan_t5_local.py
```

### âœ… SuperPOD (via SLURM)

```bash
sbatch scripts/train_flan_t5.slurm
```

---

## ğŸ” Evaluation Pipeline

Run evaluation on test set:

```bash
python tests/evaluate_on_testset.py
```

This will:

- Run inference using best model
- Save results to `datasets/predicted_answers_local.jsonl`
- Compute metrics and save to `tests/test_results/`

---

## ğŸ“Š Metrics

Evaluation includes:

- BLEU
- ROUGE-L
- METEOR
- Exact match accuracy (optional)

Computed using `src/compute_metrics.py`.

---

## ğŸ§  Prompt Format Example

Each input is reformulated as:

```text
Answer the following software engineering interview question:
You are answering a Medium General Programming question.
Question: What is polymorphism?
```

---

## âœ… Future Work

- [ ] Extend model to `flan-t5-xl`
- [ ] Collect more questions from diverse domains (DevOps, Testing, Frontend)
- [ ] Build interactive web demo for real-time QA
- [ ] Explore RLHF-based finetuning

---

## ğŸ“¬ Contact

- Ziling Wang: zwanglm@connect.ust.hk  
- Zhicheng Li: zlicv@connect.ust.hk
